{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network by Steps Using TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20191002\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "x_data = [\n",
    "          [1,2,1], [1,3,2], [1,3,4], [1,5,5], \n",
    "          [1,7,5], [1,2,5], [1,6,6], [1,7,7]\n",
    "         ]\n",
    "y_data = [\n",
    "          [0,0,1], [0,0,1], [0,0,1], [0,1,0], \n",
    "          [0,1,0], [0,1,0], [1,0,0], [1,0,0]\n",
    "         ]\n",
    "\n",
    "# Testing data\n",
    "x_test = [[2,1,1], [3,1,2], [3,3,4]]\n",
    "y_test = [[0,0,1], [0,0,1], [0,0,1]]\n",
    "\n",
    "# Placeholder for inputs and labels\n",
    "X = tf.keras.backend.placeholder([None, 3], dtype=\"float32\")\n",
    "Y = tf.keras.backend.placeholder([None, 3], dtype=\"float32\")\n",
    "\n",
    "# Weight\n",
    "W = tf.Variable(tf.keras.backend.random_normal([3, 3]))\n",
    "# Bias\n",
    "b = tf.Variable(tf.keras.backend.random_normal([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax_3:0' shape=(None, 3) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.keras.backend.log(hypothesis), axis=1))\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x7f032252d358>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_5:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples.tutorials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e77c194943c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Tensorflow already incldues MNNIST data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Tensorflow already incldues MNNIST data set\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Get input data as one_hot encoding format\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "# Labels: 0 ~ 9\n",
    "nb_labels = 10\n",
    "\n",
    "# MNIST data image of shape 28 x 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 ~ 9 digits recofnition = 10 labels\n",
    "Y = tf.placeholder(tf.float32, [None, nb_labels])\n",
    "# Weight\n",
    "W = tf.Variable(tf.random_normal([784, nb_labels]))\n",
    "# Bias\n",
    "b = tf.Variable(tf.random_normal([nb_labels]))\n",
    "\n",
    "# Hypothesis - softmax\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "# Cost\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis =1))\n",
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(\\\n",
    "                    learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), \\\n",
    "                        tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Epoch - How many times data will be trained\n",
    "training_epochs = 15\n",
    "# Batch - How many data will be trained at once\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        # Iteration - (the number of data) / (batch size).\n",
    "        max_iteration = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for itr in range(max_iteration):\n",
    "            batch_xs, batch_ys = \\\n",
    "                    mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], \n",
    "                        feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / max_iteration\n",
    "            \n",
    "        print(\"Epoch: {0:4d}, Cost: {1:0.9f}\".format(\\\n",
    "                        epoch + 1, avg_cost))\n",
    "        \n",
    "    print(\"Learning finished\")\n",
    "    \n",
    "    # Test the model using test sets\n",
    "    # accuracy.eval() == sess.run()\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, \\\n",
    "                        feed_dict={X: mnist.test.images, \\\n",
    "                                   Y: mnist.test.labels}))\n",
    "    \n",
    "    # Get on and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    # mnist.test.labels are one-hot encoded\n",
    "    print(\"Label: {0}\".format(\\\n",
    "            sess.run(tf.argmax(mnist.test.labels[r:r+1], 1))))\n",
    "    print(\"Prediction: {0}\".format(\\\n",
    "            sess.run(tf.argmax(hypothesis, 1), \\\n",
    "            feed_dict = {X: mnist.test.images[r:r+1]})))\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28),\n",
    "               cmap=\"Greys\", interpolation=\"nearest\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer Neural Network for MNIST\n",
    "By using batch with size 100, row of input layer and output layer is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Single layer neural network\n",
    "class SingleLayerNeuralNetwork():\n",
    "    def __init__(self, input_size, \\\n",
    "                       output_size, \\\n",
    "                       weight_init_std=0.01):\n",
    "        # Dictionary for weights and bias\n",
    "        self._params = {}\n",
    "        # randn returns a sample \\\n",
    "        #  from the standard normal distribution which is 1.\n",
    "        # Multipying with weight_init_std makes \\\n",
    "        #  random number array \\\n",
    "        #  whose standard variation is weight_init_std.\n",
    "        self._params['W'] = weight_init_std * \\\n",
    "                    np.random.randn(input_size, output_size)\n",
    "        # Set zero to all biases\n",
    "        self._params[\"b\"] = np.zeros(output_size)\n",
    "\n",
    "    # Sigmoid function as an active function\n",
    "    def sigmoid(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    # Softmax function as an output function\n",
    "    def softmax(self, X):\n",
    "        ret = None\n",
    "        if x.ndim == 2:\n",
    "            X = X.T\n",
    "            X = X - np.max(X, axis=0)\n",
    "            Y = np.exp(x) / np.sum(np.exp(X), axis=0)\n",
    "            ret = Y.T\n",
    "        else:\n",
    "            # To avoid overflow\n",
    "            X = X - np.max(X)\n",
    "            ret = np.exp(X) / np.sum(np.exp(X))\n",
    "    \n",
    "        return ret\n",
    "    \n",
    "    # Cross entropy error function\n",
    "    def cross_entropy_error(self, Y, labels): \n",
    "        # Translate one-hot encoded labels to answer index.\n",
    "        labels = labels.argmax(axis=1)\n",
    "        \n",
    "        batch_size = Y.shape[0]\n",
    "        log_val = np.log(Y[np.arange(batch_size), labels])\n",
    "        return -np.sum(log_val) / batch_size\n",
    "\n",
    "    # Prediction function\n",
    "    def predict(self, X):\n",
    "        W = self._params[\"W\"]\n",
    "        b = self._params[\"b\"]\n",
    "\n",
    "        # Logit\n",
    "        logit = np.dot(X, W) + b\n",
    "        hypothesis = self.sigmoid(logit)\n",
    "        output = self.softmax(hypothesis)\n",
    "\n",
    "        return output\n",
    "\n",
    "    # Cost function\n",
    "    def cost(self, X, labels):\n",
    "        Y = self.predict(X)\n",
    "\n",
    "        return self.cross_entropy_error(Y, labels)\n",
    "\n",
    "    # Accuracy function\n",
    "    def accuracy(self, X, labels):\n",
    "        Y = self.predict(X)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "        labels = np.argmax(labels, axis=1)\n",
    "\n",
    "        accuracy = np.sum(Y == labels) / float(X.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # Numerical gradient descent function\n",
    "    def numerical_gradient(self, f, X):\n",
    "        h = 1e-4\n",
    "        # Create array which is filled \n",
    "        #  with zero and whose size is the same as X\n",
    "        grad = np.zeros_like(X)\n",
    "\n",
    "        # Create iterator for numpy arrady\n",
    "        it = np.nditer(X, \\\n",
    "                       flags=[\"multi_index\"], \\\n",
    "                       op_flags=[\"readwrite\"])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = X[idx]\n",
    "\n",
    "            X[idx] = float(tmp_val) + h\n",
    "            fxh1 = f(X)\n",
    "\n",
    "            X[idx] = tmp_val - h\n",
    "            fxh2 = f(X)\n",
    "\n",
    "            grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "\n",
    "            X[idx] = tmp_val\n",
    "            it.iternext()\n",
    "\n",
    "        return grad\n",
    "\n",
    "    # Gradient function\n",
    "    def gradient(self, X, labels):\n",
    "        cost_W = lambda W: self.cost(X, labels)\n",
    "\n",
    "        grads = {}\n",
    "        grads[\"W\"] = self.numerical_gradient(cost_W, self._params[\"W\"])\n",
    "        grads[\"b\"] = self.numerical_gradient(cost_W, self._params[\"b\"])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    # Update Weight and bias\n",
    "    def update(self, grad, learning_rate):\n",
    "        for key in (\"W\", \"b\"):\n",
    "            self._params[key] -= learning_rate * grad[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
