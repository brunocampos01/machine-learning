{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de ativação é análoga à acumulação de potencial elétrico em neurônios biológicos que disparam quando um determinado potencial de ativação é atingido.\n",
    "\n",
    "<img src=\"reports/activate_principles.png\" align=\"center\" height=auto width=80%/>\n",
    "\n",
    "- As activate functions adicionam um fator de **não linearidade** nos modelos lineares\n",
    "\n",
    "### Non-Linear Activate Functions\n",
    "ao introduzir uma ativação não linear, a superfície de custo da rede neural deixa de ser uma hipébole convexa, tornando a otimização mais complicada. Isso causa algumas dificuldades de treinamento\n",
    "\n",
    "https://matheusfacure.github.io/2017/07/10/problemas-treinamento/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Activate Function Types\n",
    "- Sigmoide\n",
    "- TanH\n",
    "- ReLU\n",
    "- softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sigmoide**\n",
    "\n",
    "Como neurônios biológicos funcionam de foma binária (ativando vs não ativando), a função sigmoide é uma boa forma de modelar esse comportamento, já que assume valores apenas entre 0 (não ativação) e 1 (ativação). \n",
    "\n",
    "\\sigma(x)=\\frac{1}{1+e^x}  \\quad \\quad  \\sigma'(x)=\\sigma(x)(1-\\sigma(x))\n",
    "\n",
    "<img src=\"reports/graph_sigmoide.png\" align=\"center\" height=auto width=60%/>\n",
    "\n",
    "Os sigmóides ainda são usados ​​como funções de saída para classificação binária, mas geralmente não são usados ​​em camadas ocultas. Uma versão multidimensional do sigmóide é conhecida como função softmax e é usada para classificação em várias classes.\n",
    "\n",
    "#### Por que não usar sigmoide functions?\n",
    "- Observando o gráfico é possível ver que sigmóides não são centralizados em zero; \n",
    "- se olharmos sua derivada, podemos ver que ela satura para valores de -5 até 5, ou seja, ela tende a zero.\n",
    "- outro problema é que a derivada da função sigmoide é sempre < 1 (Vanishing Gradient Problem)\n",
    "- No geral, a função logística praticamente desapareceu dos modelos modernos de redes neurais mais convencionais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TanH**\n",
    "\n",
    "\n",
    "Similar a função sigmoide, a função Tangente Hiperbólica (TanH) também tem um formato de ‘S’, mas varia de -1 a 1, em vez de 0 a 1 como na sigmoide. A TanH se aproxima mais da identidade, sendo assim uma alternativa mais atraente do que a sigmoide para servir de ativação às camadas ocultas das RNAs. A TanH sua derivada são dadas, respectivamente, por:\n",
    "\n",
    "http://www.sciweavers.org/free-online-latex-equation-editor\n",
    "\\\\[ tanh(x)=2\\sigma(2x) - 1   \\quad \\quad  tanh'(x)=1 - tanh^2(x) ]\\\\\n",
    "\n",
    "<img src=\"reports/graph_tanh.png\" align=\"center\" height=auto width=60%/>\n",
    "\n",
    "- A derivada é maior, chegando ao máximo de 1 quando x=0. Por esse motivo, quando uma função sigmoidal precisa ser utilizada, recomenda-se a TanH no lugar da sigmoide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ReLU Activate**\n",
    "_rectified linear unit (ReLU)_\n",
    "\n",
    "% <![CDATA[\n",
    "ReLU(x)=max\\{0, x\\}   \\quad \\quad  ReLU'(x)=\n",
    "\t\\begin{cases}\n",
    "    \t1, & \\text{se } x\\ge 0\\\\\n",
    "    \t0, & \\text{c.c.}\n",
    "\t\\end{cases} %]]>\n",
    "    \n",
    "<img src=\"reports/graph_relu.png\" align=\"center\" height=auto width=60%/>\n",
    "\n",
    "Redes com a função ReLU são fáceis de otimizar, já que a ReLU é extremamente parecida com a função identidade. A única diferença é que a ReLU produz zero em metade do seu domínio. \n",
    "\n",
    "\n",
    "Quase todos os modelos de deep learning usam ReLU hoje em dia. No entanto, a ReLU deve ser usada apenas dentro de hidden layer de uma rede neural, e não para a camada de saída - que deve ser sigmóide para classificação binária, softmax para classificação multiclasse e linear para um problema de regressão.\n",
    "\n",
    "\n",
    "To prevent neural network from wrong training, sigmoid should be used as the activation function in output layer. For the activation function of hidden layer, ReLU is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "class Relu():\n",
    "    def __init__(self):\n",
    "        self._mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self._mask = (X <= 0)\n",
    "        out = X.copy()\n",
    "        out[self._mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, d):\n",
    "        d[self._mask] = 0\n",
    "        dx = d\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Function\n",
    "_Simply speaking, softmax function returns probability of the label_\n",
    "\n",
    "S(y_i) = \\frac{e^{y_i}}{\\sum_j e^{y_j}}\n",
    "\n",
    "softmax.png\n",
    "- É uma função de scaling (normalized exponential function) que converte vetor de k-dimensões em um valor probabilistico entre (0, 1). \n",
    "- Usado no final de uma neural network\n",
    "- Recomendado quando houve treinamento com cross-entropy\n",
    "- Usar somente em **classificação multipla**\n",
    "\n",
    "- Implemeantation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.38905609893065, 2.718281828459045, 1.1051709180756477]\n"
     ]
    }
   ],
   "source": [
    "logits = [2.0, 1.0, 0.1] \n",
    "import numpy as np \n",
    "exps = [np.exp (i) for i in logits]\n",
    "print(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Softmax function\n",
    "def softmax(a):\n",
    "    max_a = np.max(a)   # To prevent overflow\n",
    "                        # Exponential could have too large value.\n",
    "                        # max_a will be downscale the original value.\n",
    "    exp_a = np.exp(a - max_a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "x = np.random.rand(5,1)\n",
    "y = softmax(x)\n",
    "\n",
    "print(\"    {0:7}   {1:7}\".format(\"X\", \"S(X)\"))\n",
    "for i in range(len(x)):\n",
    "    print(\"    {0:7.6f}  {1:7.6f}\".format(x[i][0], y[i][0]))\n",
    "print(\"Sum {0:7.6f}  {1:7.6f}\".format(np.sum(x), np.sum(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax-With-Loss\n",
    "class SoftmaxWithLoss():\n",
    "    def __init__(self):\n",
    "        self._loss = None\n",
    "        self._Y = None\n",
    "        self._labels = None\n",
    "\n",
    "    def _softmax(self, X):\n",
    "        if X.ndim == 2:\n",
    "            X = X.T\n",
    "            X = X - np.max(X, axis=0)\n",
    "            Y = np.exp(X) / np.sum(np.exp(X), axis=0)\n",
    "            return Y.T\n",
    "\n",
    "        # Protect from overflow\n",
    "        X = X - np.max(x)\n",
    "        return np.exp(X) / np.sum(np.exp(X))\n",
    "\n",
    "    def _cross_entropy_error(self, Y, labels):\n",
    "        if Y.ndim == 1:\n",
    "            labels = labels.reshape(1, labels.size)\n",
    "            Y = Y.reshape(1, Y.size)\n",
    "\n",
    "        # If train data is one-hot encoded,\n",
    "        #  translate it to answer label\n",
    "        if labels.size == Y.size:\n",
    "            labels = labels.argmax(axis=1)\n",
    "\n",
    "        batch_size = Y.shape[0]\n",
    "\n",
    "        log_val = np.log(Y[np.arange(batch_size), labels])\n",
    "        return -np.sum(log_val) / batch_size\n",
    "\n",
    "    def forward(self, X, labels):\n",
    "        self._labels = labels\n",
    "        self._Y = self._softmax(X)\n",
    "        self._loss = self._cross_entropy_error(self._Y, self._labels)\n",
    "\n",
    "        return self._loss\n",
    "\n",
    "    def backward(self, d=1):\n",
    "        batch_size = self._labels.shape[0]\n",
    "        # If train data is one-hot encoded,\n",
    "        #  translate it to answer label\n",
    "        if self._labels.size == self._Y.size:\n",
    "            dx = (self._Y - self._labels) / batch_size\n",
    "        else:\n",
    "            dx = self._Y.copy()\n",
    "            dx[np.arange(batch_size), self._labels] -= 1\n",
    "            dx = dx / batch_size\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summary About Activate and Cost Function\n",
    "\n",
    "Existem funções de cost específicas que devem ser usadas em cada cenários, compatíveis com o tipo de output da neural network. \n",
    "\n",
    "A summary of the data types, distributions, output layers, and cost functions are given in the table below.\n",
    "\n",
    "<img src=\"reports/distr_cost_output_layer.png\" align=\"center\" height=auto width=60%/>\n",
    "\n",
    "| Problem  | Hidden Layer | Output Layer |\n",
    "|----------|--------------|--------------|\n",
    "| Linear   | Identity     | Identity     |\n",
    "| binary classification | ReLU         | Sigmoid      |\n",
    "| multi classification  | ReLU         | Softmax      |\n",
    "\n",
    "\n",
    " Por exemplo, usar o MSE em dados binários faz muito pouco sentido e, portanto, para dados binários, usamos a função de perda de entropia cruzada binária.\n",
    " \n",
    " \n",
    "\n",
    "<img src=\"reports/comparation.png\" align=\"center\" height=auto width=60%/>\n",
    "\n",
    "**ELU > leaky ReLU > ReLU >> tanh > sigmoide.**\n",
    "\n",
    "**NOTE**:\n",
    "- **Sigmoide** usar somente na layer de output em casos de **classificação binária.**\n",
    "- **Softmax** usar somente na layer de output em casos de **classificação multipla**. \n",
    "- ReLU deve ser usada apenas dentro de hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [1] https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
