{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier ##\n",
    "\n",
    "In this mission we will be using the Naive Bayes algorithm to create a model that can classify [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) SMS messages as spam or not spam, based on the training we give to the model.\n",
    "\n",
    "## Overview\n",
    "\n",
    "   This project has been broken down in to the following steps: \n",
    "\n",
    "**1. Step 1: introduction** \n",
    "   * Import the dataset\n",
    "   * Rename the column\n",
    "   * Print the first five values\n",
    "   \n",
    "**2. Step 2: Data cleaning**\n",
    "   * Convert the values in the 'label' column to numerical values\n",
    "   * Get size of the dataset\n",
    "   * Convert all strings to lower case\n",
    "   * Removing all punctuations\n",
    "   * Tokenization\n",
    "   * Count frequencies\n",
    "   \n",
    "**3. Step 3: Data cleaning from scratch**\n",
    "   * Import the `sklearn.feature_extraction.text.CountVectorizer`\n",
    "   * Fit dataset\n",
    "   * Create a matrix and convert to array\n",
    "   * Convert the array into a dataframe\n",
    "   * Set the column names with name words\n",
    "   \n",
    "**4. Step: Training and testing sets**\n",
    "   * Split the dataset into a training and testing set\n",
    "   * Naive Bayes implementation using scikit-learn\n",
    "   \n",
    "**5. Step: Evaluating our model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** Part 1 - Instructions: **\n",
    "* Import the dataset into a pandas dataframe.\n",
    "* Rename the column names.\n",
    "* Print the first five values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/brunocampos01/projetos/data_science/machine_learning/supervised_learning/data_base/smsspamcollection/SMSSpamCollection' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0ce2817e2b22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m df = pd.read_table('/home/brunocampos01/projetos/data_science/machine_learning/'\n\u001b[1;32m      7\u001b[0m                         \u001b[0;34m'supervised_learning/data_base/smsspamcollection/SMSSpamCollection'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                    names=['label', 'sms_message'])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Print the first five values of the dataframe with the new column names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/venv_global/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/venv_global/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/venv_global/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/venv_global/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/venv_global/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/home/brunocampos01/projetos/data_science/machine_learning/supervised_learning/data_base/smsspamcollection/SMSSpamCollection' does not exist"
     ]
    }
   ],
   "source": [
    "#Import the dataset into a pandas dataframe using the read_table method.\n",
    "import pandas as pd\n",
    "\n",
    "# Rename the column names by specifying a list \n",
    "# ['label, 'sms_message'] to the 'names' argument of read_table().\n",
    "df = pd.read_table('/home/brunocampos01/projetos/data_science/machine_learning/'\n",
    "                        'supervised_learning/data_base/smsspamcollection/SMSSpamCollection', \n",
    "                   names=['label', 'sms_message'])\n",
    "\n",
    "# Print the first five values of the dataframe with the new column names.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing #\n",
    "\n",
    "Now that we have a basic understanding of what our dataset looks like, lets convert our labels to binary variables:\n",
    "* 0 to represent 'ham'(i.e. not spam) \n",
    "* 1 to represent 'spam' for ease of computation. \n",
    "\n",
    "You might be wondering why do we need to do this step? The answer to this lies in how scikit-learn handles inputs. Scikit-learn only deals with numerical values and hence if we were to leave our label values as strings, scikit-learn would do the conversion internally(more specifically, the string labels will be cast to unknown float values).\n",
    "***\n",
    "*Você pode estar se perguntando por que precisamos fazer este passo? A resposta para isso está em como o scikit-learn manipula entradas. O Scikit-learn lida apenas com valores numéricos e, portanto, se deixássemos nossos valores de rótulo como strings, o scikit-learn faria a conversão internamente (mais especificamente, os rótulos de string serão convertidos em valores flutuantes desconhecidos).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 2 - Instructions: **\n",
    "* **Convert the values in the 'label' column to numerical values** using map method as follows:\n",
    "{'ham':0, 'spam':1} This maps the 'ham' value to 0 and the 'spam' value to 1.\n",
    "* Print out number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values in the 'label'\n",
    "dict =  {'ham':0, 'spam':1}\n",
    "\n",
    "# mapped collumn to dict \n",
    "df ['label'] = df.label.map(dict)\n",
    "\n",
    "# print out number of rows and columns using 'shape'.\n",
    "df.shape\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words ###\n",
    "Most ML algorithms rely on numerical data to be fed into them as input, and email messages are usually text.\n",
    "\n",
    "The basic idea of BoW is to take a piece of text and count the frequency of the words in that text. It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter.\n",
    "\n",
    "###### Matrix words (tokens)\n",
    "We can convert a collection of documents to a matrix. A row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document.\n",
    "\n",
    "Lets say we have 4 documents as follows:\n",
    "\n",
    "`['Hello, how are you!',\n",
    "'Win money, win from home.',\n",
    "'Call me now',\n",
    "'Hello, Call you tomorrow?']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('/home/brunocampos01/projetos/data_science/images/bayes_spam.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break this down and see how we can do this conversion using a small set of documents.\n",
    "\n",
    "To handle this, we will be using sklearns \n",
    "[count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) method which does the following:\n",
    "\n",
    "* It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.\n",
    "* It counts the occurrence of each of those tokens.\n",
    "***\n",
    "Para lidar com isso, usaremos o método vectorizador de contagem sklearns, que faz o seguinte:\n",
    "\n",
    "* Ele tokeniza a string (separa a string em palavras individuais) e dá um ID inteiro a cada token.\n",
    "* Conta a ocorrência de cada um desses tokens.\n",
    "***\n",
    "\n",
    "** Please Note: ** \n",
    "\n",
    "* `lowercase`: The CountVectorizer method automatically converts all tokenized words to their lower case. It does this using the `lowercase` parameter which is by default set to `True`.\n",
    "\n",
    "* `token_pattern`: It also ignores all punctuation.\n",
    "\n",
    "* `stop_words` Stop words refer to the most commonly used words in a language.By setting this parameter value to `english`, CountVectorizer will automatically ignore all words(from our input text) that are found in the built in list of english stop words in scikit-learn. This is extremely helpful as stop words can skew our calculations when we are trying to find certain key words that are indicative of spam.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 3 - Instructions: **\n",
    "* Convert all the strings in the documents set to their lower case. Save them into a list called 'lower_case_documents'. You can convert strings to their lower case in python by using the lower() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all strings to lower case\n",
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "lower_case_documents = []\n",
    "\n",
    "for word in documents:\n",
    "    lower_case_documents.append(word.lower())\n",
    "print(lower_case_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 4 - Instructions: **\n",
    "* Remove all punctuation from the strings in the document set. Save them into a list called \n",
    "'sans_punctuation_documents'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all punctuations\n",
    "import string\n",
    "\n",
    "sans_punctuation_documents = []\n",
    "\n",
    "for word in lower_case_documents:\n",
    "    # method translate: str.translate(table[, deletechars]);\n",
    "    word_translate = word.translate(str.maketrans('', '', string.punctuation))\n",
    "    sans_punctuation_documents.append(word_translate)\n",
    "print(sans_punctuation_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 5 - Instructions:**\n",
    "* Tokenize the strings stored in 'sans_punctuation_documents' using the split() method. and store the final document set \n",
    "in a list called 'preprocessed_documents'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "preprocessed_documents = []\n",
    "\n",
    "for word in sans_punctuation_documents:\n",
    "    tk = word.split()\n",
    "    preprocessed_documents.append(tk)\n",
    "print(\"Preprocessed documents: \\n\", preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 6 - Instructions:**\n",
    "* Using the **Counter()** (import collections) method and preprocessed_documents as the input, create a dictionary with the keys being each word in each document and the corresponding values being the frequency of occurrence of that word. Save each Counter dictionary as an item in a list called 'frequency_list'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequencies\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "frequency_list = []\n",
    "\n",
    "for word in preprocessed_documents:\n",
    "    # Counter() insert once word in dict with counter\n",
    "    frequency_count = Counter(word)\n",
    "    frequency_list.append(frequency_count)\n",
    "        \n",
    "pprint.pprint(\"Count words: {}\".format(frequency_list))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words in scikit-learn###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 7 - Instructions:**\n",
    "Import the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "print(count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 8 - Instructions:**\n",
    "Fit your document dataset to the CountVectorizer object you have created using fit(), and get the list of words \n",
    "which have been categorized as features using the get_feature_names() meth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit dataset\n",
    "count_vector.fit(documents)\n",
    "\n",
    "# print words\n",
    "count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 9 - Instructions:**\n",
    " * Create a matrix with the rows being each of the 4 documents, and the columns being each word. \n",
    "The corresponding (row, column) value is the frequency of occurrence of that word(in the column) in a particular\n",
    "document(in the row). You can do this using the **transform()** method and passing in the document data set as the argument.The transform() method returns a matrix of numpy integers, you can convert this to an array using\n",
    "**toarray()**. Call the array 'doc_array'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document with Tokenization into array\n",
    "doc_array = count_vector.transform(documents)\n",
    "print(type(doc_array))\n",
    "\n",
    "# convert int to array\n",
    "doc_array = doc_array.toarray()\n",
    "print(type(doc_array))\n",
    "print()\n",
    "print(doc_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 10 - Instructions:**\n",
    "* Convert the array we obtained, loaded into 'doc_array', into a dataframe\n",
    "* Set the column names to the word names(which you computed earlier using get_feature_names(). \n",
    "* Call the dataframe 'frequency_matrix'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "frequency_matrix = pd.DataFrame(doc_array, columns = count_vector.get_feature_names())\n",
    "\n",
    "print(frequency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training and testing sets ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Instructions:**\n",
    "First step in this regard would be to **split our dataset into a training and testing.**\n",
    "Use the following variables:\n",
    "* `X_train` is our training data for the 'sms_message' column.\n",
    "* `y_train` is our training data for the 'label' column\n",
    "* `X_test` is our testing data for the 'sms_message' column.\n",
    "* `y_test` is our testing data for the 'label' column\n",
    "Print out the number of rows we have in each our training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], \n",
    "                                                    df['label'], \n",
    "                                                    random_state=1)\n",
    "\n",
    "print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Have data split in training and test.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Part 12 - Instructions:**\n",
    "* Applying again Bag of Words processing\n",
    "* Convert matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **First:** fit our training data (`X_train`) into `CountVectorizer()` and return the matrix.\n",
    "\n",
    "* **Second:** transform our testing data (`X_test`) to return the matrix. \n",
    "* Vectorizing our dataset\n",
    "    \n",
    "Note that `X_train` is our training data for the 'sms_message' column in our dataset and we will be using this to train our model. \n",
    "\n",
    "`X_test` is our testing data for the 'sms_message' column and this is the data we will be using(after transformation to a matrix) to make predictions on. We will then compare those predictions with `y_test` in a later step.    \n",
    "***\n",
    "Observe que X_train são nossos dados de treinamento para a coluna 'sms_message' em nosso conjunto de dados e usaremos isso para treinar nosso modelo.\n",
    "\n",
    "X_test é nossos dados de teste para a coluna 'sms_message' e são os dados que usaremos (após a transformação em uma matriz) para fazer previsões. Em seguida, compararemos essas previsões com y_test em uma etapa posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing our dataset\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# fit train, then use it to create a document-term matrix\n",
    "X_train_dtm = vect.fit(X_train)\n",
    "\n",
    "# transform train\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "#print(X_train_dtm)\n",
    "\n",
    "# transform test\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "#print(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. There are various mechanisms for doing so, but first let's do quick recap of them.\n",
    "\n",
    "** Accuracy ** measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "** Precision ** tells us what proportion of messages we classified as spam, actually were spam.\n",
    "It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "`[True Positives/(True Positives + False Positives)]`\n",
    "\n",
    "** Recall(sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam.\n",
    "It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of\n",
    "\n",
    "`[True Positives/(True Positives + False Negatives)]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
