{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure of Learning by Reinforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Políticas\n",
    "- Funções estado-valor\n",
    "- Equações de Bellman - Parte 1\n",
    "- Otimalidade\n",
    "- Funções ação-valor\n",
    "- Políticas ótimas\n",
    "- Equações de Bellman - Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "A Policy determina como um agente escolhe uma **ação (A)** como resposta ao **estado atual (S)**. \n",
    "\n",
    "Por isso, uma políta nada mais é do que um mapeamento de um conjunto de estados do ambiente para um conjunto de possíveis ações.\n",
    "\n",
    "<img src=\"images/policy.png\" />\n",
    "**π: S→A<br/>**\n",
    "Example:<br/>\n",
    "π(low) = recharge<br/>\n",
    "π(high) = search\n",
    "\n",
    "Há tambem a _stochastic policy_ que vai determinar uma ação aleatória para o agente.\n",
    "\n",
    "<img src=\"images/stochastic_policy.png\" />\n",
    "Este tipo de política retorna uma probabilidade de ações que o agente pode tomar em um determinado estado (S)\n",
    "\n",
    "### Example)\n",
    "<img src=\"images/example_policy.png\" />\n",
    "\n",
    "- se o nível de bateria for baixo, o agente recarrega a bateria com 50% de probabilidade, aguarda por latas com 40% de probabilidade, e busca por latas com 10% de probabilidade.\n",
    "- se o nível de bateria for alto, o agente busca por latas com 90% de probabilidade e aguarda por latas com 10% de probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando a melhor política\n",
    "\n",
    "Para isso precisamos usar um exemplo de Gridword.<br/>\n",
    "Considere um mundo pequeno sendo estes 9 quadrados de grama e um agente (bola)<br/>\n",
    "Cada quadrado deste mundo é um estado(S)\n",
    "\n",
    "<img src=\"images/gridword.png\" />\n",
    "É dado alguns tipos de movimentos possíveis para o agente<br/>\n",
    "O agente tem um objetivo que é chegar o mais rápido possível no canto inferior esquerdo<br/>\n",
    "Tarefa episídica\n",
    "<img src=\"images/gridword_moviment.png\" />\n",
    "- Para a maioria dos movimentos ele recebe Reward = -1\n",
    "- Se o movimento levar para as montanhas ele recebe Reward = -3\n",
    "- O objetivo vale Reward = 5 para encorajar o agente a chegar o mais rápido possível"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-Value Function\n",
    "\n",
    "Uma state-value function serve para uma policy prever uma reward.\n",
    "<img src=\"images/state-value.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equations\n",
    "\n",
    "É muito difícil garantirmos com certeza uma recompensa imediata mas podemos garantir uma expectativa.\n",
    "\n",
    "A Equação de Expectância de Bellman expressa o valor de qualquer estado **s** em termos da recompensa imediata esperada e o valor esperado do próximo estado:\n",
    "\n",
    "É uma equação que calcula a espectativa de reward\n",
    "<img src=\"images/bellman_equation.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Dê a melhor stochastic policy PI e determine value(state) para todos os estados.\n",
    "<img src=\"images/example.png\" />\n",
    "\n",
    "**Resolução**<br/>\n",
    "Se é uma stochastic policy então o agente escolhe suas ações de modo aleatório.<br/>\n",
    "Na imagem abaixo há a stochastic policy:\n",
    "<img src=\"images/example_1.png\" />\n",
    "\n",
    "Agora é preciso determinar a State-Value Function correspondente a esta _policy_.\n",
    "<img src=\"images/example_2.png\" />\n",
    "<img src=\"images/example_3.png\" />\n",
    "\n",
    "**Observação:** este exemplo serve para ilustrar o fato de que é possível resolver o sistema de equações dado pela equação de expectância de Bellman para π diretamente. Entretanto, na prática, e especialmente para processos de decisão de Markov (MDPs) bem maiores, iremos usar, em vez disso, uma abordagem de solução iterativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
