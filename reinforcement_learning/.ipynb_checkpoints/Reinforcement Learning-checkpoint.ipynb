{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A palavra _reinforcement_ em reinforcement learning vem do behavorismo. Ele se refere a um estimo logo após o comportamento e esse comportamento é levado adiante para aumentar as chances de o estimo ocorrer no futuro.\n",
    "\n",
    "\n",
    "O aluno não é informado de quais ações tome, mas em vez disso, deve descobrir quais ações produzem mais recompensa, tentando-as.\n",
    "\n",
    "#### Estas duas características - _trial-and-error search and delayed reward_ - são as duas mais importantes características distintivas da aprendizagem por reforço.\n",
    "\n",
    "## unsupervised learning X reinforcement learning\n",
    "Embora o aprendizado por reforço possa ser visto como um tipo de aprendizado não supervisionado\n",
    "porque não se baseia em exemplos de comportamento correto, ele não é pois, o aprendizado por reforço está tentando maximizar um sinal de recompensa em vez de tentar encontrar estrutura oculta.\n",
    "\n",
    "Descobrindo estrutura na experiência de um agente pode certamente ser útil no aprendizado por reforço, mas\n",
    "em si não aborda o problema da aprendizagem de reforço da maximização de um sinal de recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reinforcement learning agent\n",
    "um agente de aprendizagem de reforço deve preferir ações que tentou no passado e descobriu-se ser eficaz na produção de recompensa.\n",
    "\n",
    "O agente tem que explorar o que já tem\n",
    "experiente, a fim de obter recompensa, mas também tem que explorar a fim de fazer melhor\n",
    "seleções de ação no futuro.\n",
    "\n",
    "O agente deve tentar uma variedade de ações e progressivamente favorecer aqueles que parecem ser os melhores. Em uma tarefa estocástica, cada a ação deve ser tentada muitas vezes para obter uma estimativa confiável de sua recompensa esperada.\n",
    "\n",
    "Outra característica fundamental da aprendizagem por reforço é que ela considera explicitamente\n",
    "problema de um agente direcionado por objetivos interagindo com um ambiente incerto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "- A master chess player makes a move. The choice is informed both by planning—\n",
    "anticipating possible replies and counterreplies—and by immediate, intuitive judgments\n",
    "of the desirability of particular positions and moves.\n",
    "\n",
    "- Um bezerro se esforça para ficar em pé minutos depois de nascer. Meia hora depois esta correndo a 100km/h. :()\n",
    "\n",
    "- Phil prepara seu café da manhã. Intimamente examinado, mesmo este aparentemente mundano\n",
    "atividade revela uma teia complexa de comportamento condicional e meta de intertravamento\n",
    "relacionamentos: caminhando até o armário, abrindo-o, selecionando uma caixa de cereal,\n",
    "procurando, agarrando e recuperando a caixa. Outros complexos, ajustados, interativos\n",
    "seqüências de comportamento são necessárias para obter uma tigela, colher e caixa de leite. Cada\n",
    "passo envolve uma série de movimentos oculares para obter informações e orientar\n",
    "e locomoção. Julgamentos rápidos são feitos continuamente sobre como\n",
    "objetos ou se é melhor para transportar alguns deles para a mesa de jantar antes\n",
    "obtendo outros. Cada passo é guiado por objetivos, como pegar uma colher ou pegar\n",
    "para a geladeira, e está em serviço de outros objetivos, como ter a colher para comer\n",
    "com uma vez o cereal é preparado e, finalmente, obter nutrição. Se\n",
    "ele está ciente disso ou não, Phil está acessando informações sobre o estado de seu corpo\n",
    "que determina suas necessidades nutricionais, nível de fome e preferências alimentares\n",
    "\n",
    "- Carros autônomos\n",
    "- Game Players autônomos\n",
    "- Robô aprendendo a andar\n",
    "\n",
    "<img src=\"images/goals.png\" />\n",
    "\n",
    "Todos envolvem interação entre um agente ativo de tomada de decisão e seu ambiente, dentro de\n",
    "que o agente procura atingir uma **goal**, apesar da incerteza sobre seu ambiente.\n",
    "\n",
    "Ao mesmo tempo, em todos esses exemplos, os efeitos das ações não podem ser totalmente previstos;\n",
    "Assim, o agente deve monitorar seu ambiente com freqüência e reagir apropriadamente. \n",
    "\n",
    "\n",
    "Em todos esses exemplos, o agente pode usar sua experiência para melhorar seu desempenho\n",
    "ao longo do tempo. O jogador de xadrez refina a intuição que ele usa para avaliar posições,\n",
    "melhorando seu jogo; o bezerro gazela melhora a eficiência\n",
    "com o qual ele pode ser executado; Phil\n",
    "aprende a simplificar fazendo seu café da manhã. O conhecimento que o agente traz para a tarefa em\n",
    "o início - seja de experiências anteriores com tarefas relacionadas ou incorporado a ele por design ou\n",
    "evolução - influencia o que é útil ou fácil de aprender, mas a interação com o ambiente\n",
    "é essencial para ajustar o comportamento para explorar recursos específicos da tarefa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of Reinforcement Learning\n",
    "Além do agente e do ambiente, pode-se identificar quatro subelementos principais de um sistema de aprendizagem de reforço: \n",
    "- uma política ou befiel: define o modo como o agente de aprendizagem se comporta em um determinado momento\n",
    "- um sinal de recompensa: define o goal\n",
    "- uma função de valor: especifica o que é bom a longo prazo\n",
    "- uma modelagem do ambiente (opcional): é algo que imita o comportamento do ambiente, ou mais geralmente, isso permite inferências sobre como o ambiente se comportará"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure reinforcement Learning\n",
    "\n",
    "Um agente é uma forma de generalização de algo que aprende com o meio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/agent-observation-enviroment.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando um agente observa o ambiente, ele executa alguma ação para conseguir uma reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/agent-observation-enviroment-action.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geralmente supomos que o ambiente NÃO mostrará todas as informações necessárias para o agente tomar uma boa decisão.\n",
    "\n",
    "**OBS**: considere observation como um state do ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matematical Structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Um ambiente possui estado (S) e um tempo (0, 1,...).\n",
    "- Um agente realiza uma ação (A) num tempo (0, 1,...).\n",
    "\n",
    "<img src=\"images/agent-state-enviroment.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ambiente reage à ação do agente alterando seu estado.\n",
    "<img src=\"images/agent-state-enviroment_s1.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso a ação do agente tenha sido a melhor escolha possível, o ambiente lhe dará uma recompensa.\n",
    "<img src=\"images/agent-state-enviroment_s1_r1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ciclo de 3 estados forma um **episódio** e pode se repetir de modo infinito ou não.\n",
    "<img src=\"images/agent-goal.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex)\n",
    "<img src=\"images/agent-robot.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/agent-state-enviroment_ex.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episódica ou contínua?\n",
    "Lembre-se:\n",
    "\n",
    "- Uma **tarefa** é uma instância do problema da aprendizagem por reforço.\n",
    "- **Tarefas contínuas** são tarefas que continuam para sempre, sem fim.\n",
    "- **Tarefas episódicas** são tarefas com pontos de início e fim bem definidos.\n",
    "    - Neste caso, referimo-nos a uma sequência completa de interação, do começo até o final, como um **episódio**.\n",
    "    - Tarefas episódicas se encerram sempre que o agente chega a um **estado terminal**.\n",
    "    \n",
    "Ex) Considere um agente de aprendizagem por reforço que gostaria de aprender a jogar o jogo de tabuleiro Go. Esta é uma tarefa contínua ou episódica?<br/>\n",
    "O início do jogo marca o começo de um episódio, que se encerra no fim do jogo.\n",
    "\n",
    "\n",
    "Ex) Considere um agente imortal que represente um cachorrinho, que gostaria de obter o máximo possível de petiscos quanto for possível de seu dono (imortal). Esta é uma tarefa melhor formulada como contínua ou episódica?<br/> \n",
    "A interação agente-ambiente continuará sem fim, então esta é uma tarefa que seria melhor definida como contínua."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hipótese da recompensa\n",
    "<img src=\"images/rewards-hypothesis.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula of Rewards\n",
    "\n",
    "<img src=\"images/formula_reward.png\" />\n",
    "A reward é decidida a cada instante de tempo.\n",
    "\n",
    "Esta fórmula foi obtida pelo Google DeepMind no artigo https://arxiv.org/pdf/1707.02286.pdf\n",
    "\n",
    "<img src=\"images/formula_reward_details.png\" />\n",
    "\n",
    "Com isso, o robô aprendeu a andar **conscientemente** mas o objetivo dele não era _aprender a andar_ mas sim conseguir a reward.\n",
    "\n",
    "<img src=\"images/formula_reward_resolution.png\" />\n",
    "\n",
    "Através desta função o Google DeeMind provou que um robô aprende a andar de um jeito muito parecido com um humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acumullated Rewards\n",
    "\n",
    "Um agente precisa aprender a assimir os efeitos complexos que suas ações tem no envoriment. <br/><br/>\n",
    "A partir disso, por exemplo um robô que anda, se um agente sempre se concentrar na reward em todos os instatntes de tempo, ele aprenderá a escolher os melhores movimentos criados para garantir a estabilidade a longo prazo.\n",
    "\n",
    "<img src=\"images/formula_reward_slow.png\" />\n",
    "\n",
    "De inicio, o robô sacrifica um pouco de sua reward de _walk fast_ e anda um pouco mais devagar porque evitará por mais tempo que ele caia e assim garante uma rewards acumulativa.\n",
    "\n",
    "<img src=\"images/formula_reward_max_reward.png\" />\n",
    "\n",
    "Analisando até aqui percebemos que o agente sempre vai buscar o maximo de reward possível e para isso ele vai sempre **analisar o futuro**.<br/>\n",
    "Essa análise do futuro para conseguir uma maior reward cumulativa é chamada de **return (G)**\n",
    "\n",
    "<img src=\"images/formula_return.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discounted Return\n",
    "\n",
    "...\n",
    "\n",
    "<img src=\"images/formula_return_descount.png\" />\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  G_x = \\sum_{k=0}^{\\infty}gama^k*R_t\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "### Mas como definir qual valor devo descontar cada reward?\n",
    "discount rate\n",
    "\n",
    "\n",
    "<img src=\"images/formula_return_discount_gama.png\" />\n",
    "O gama não é algo aprendido pelo agente. É algo que o proprio desenvolvedor define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- http://go.udacity.com/rl-textbook\n",
    "- https://arxiv.org/pdf/1707.02286.pdf fórmula da reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
