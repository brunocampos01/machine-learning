{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# **Linear Regression**\n",
    "![png](reports/icon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "O algoritmo de _linear regression_ tem o objetivo de encontrar a melhor reta que se ajusta nos dados.\n",
    "\n",
    "\n",
    "Basicamente a ideia de uma _linear regression_ é dar pequenos espaços entre os pontos para que a reta se aproxime de todos os pontos.\n",
    "\n",
    "## Summary\n",
    "  - [Definitions](#)\n",
    "  - [Advantages](#)\n",
    "  - [Disadvantages](#)\n",
    "  - [Cost Functions](#)\n",
    "  - [Descent Gradient](#)\n",
    "  - [Regularization](#)\n",
    "  - [Linear Regressions Types](#)\n",
    "    - [Linear Regression](#)\n",
    "    - [Polinomial Regression](#)\n",
    "    - [Logistic Regression](#)\n",
    "    - [Lasso Regression](#)\n",
    "    - [Ridge Regression](#)\n",
    "  - [Examples](#)\n",
    "    - [Examples](#)\n",
    "    - [Examples](#)\n",
    "    - [Examples](#)\n",
    "    - [Examples](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Advantages\n",
    "- Otima solução quando a relação entre as variáveis dependentes(y) e independentes(x) geram uma linha. \n",
    "- Encontrar correlação entre variáveis\n",
    "- Fácil de encontrar outliers\n",
    "- Quando tentar prever um valor contínuo - como preço, demanda, ou um índice qualquer - sempre comece usando regressão linear antes de tentar outros algoritmos de AM mais complexos.\n",
    "- Mesmo com milhões de dados, é possível estimar os parâmetros em menos de um segundo. Além disso, uma vez treinado, o regressor ocupa muito pouco espaço, pois só armazena o vetor w^w^. \n",
    "- O modelo de regressão linear é de longe o mais utilizado em econometria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Disadvantages\n",
    "\n",
    "A regressão linear nem sempre é a melhor escolha dependendo da situação. Exemplos:\n",
    " - **Regressão linear funciona melhor quando os dados são lineares:** \n",
    "A regressão linear produz um modelo que é uma linha reta extraída a partir dos dados de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_30_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    " - **Regressões lineares são sensíveis a casos extraordinários:** \n",
    "A regressão linear tenta encontrar a linha que \"melhor se encaixa\" nos dados de treinamento. Caso o conjunto de dados tenha valores extremos que fujam muito do padrão geral, eles podem ter um efeito inesperadamente grande no modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_32_0.png)\n",
    "\n",
    "- Problema de overfit que gera uma lata variância\n",
    "\n",
    "![gif](reports/overfit_variance.png)\n",
    "\n",
    "![gif](reports/overfit.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_4_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **w** tambem é chamado de preditor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_5_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_6_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_7_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_8_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_9_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "A regressão linear simples é útil para prever o valor de uma variável dependente por uma variável independente. Contudo, este tipo de regressão não é muito útil no contexto real pois sempre há mais de um fator que pode influenciar uma variável dependente (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Por exemplo, no caso abaixo temos o preço das casas (y = variável dependente) que é afetado pela qualidade de ensino das escolas proximas (x1) e pelo tamanho da casa(x2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_24_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "A partir disso, observamos que o preço de uma casa (y) pode ser afetado (ser dependente) por N variáveis(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_26_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_27_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_28_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Cost Function\n",
    "- É a diferença entre o valor real e o valor predito.\n",
    "- Resíduo = erro médio absoluto\n",
    "- Cost Function = Error Function \n",
    "\n",
    "#### Cost ?\n",
    "Uma função que mapeia um evento ou valores de uma ou mais variáveis ​​para um número real, representando intuitivamente algum \"custo\" associado ao evento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](reports/variables_linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_11_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_12_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### MSE(Mean Squared Error) \n",
    "É possivel calcular um erro médio quadrático.\n",
    "\n",
    "mse.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_14_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    " - OBS: a única difrença para o que eu vi em técnicas estatíticas de predição foi a derivação da fórmula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Comparison of error types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_17_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Mean absolute error\n",
    "Indiferente da reta, sempre dará o mesmo tamanho de erro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Mean Square Error\n",
    "Neste caso temos uma função de 2º grau, onde a linha B se encontra no ponto mais baixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "![png](reports/output_20_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean absolute error x Mean Square Error\n",
    "\n",
    "- Quando usar um e outro ?\n",
    "Na prática, depende muito pois o que causa é a alteração no _learning rate_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# original data\n",
    "X = [1, 2, 3]\n",
    "y = [1, 2.5, 3.5]\n",
    "\n",
    "\n",
    "hyps = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutiply the original X values by hyps to produce hypothesis values for each X\n",
    "def multiply_matrix(mat, hyps):\n",
    "    mutated = []\n",
    "    for i in range(len(mat)):\n",
    "        mutated.append(mat[i] * hyps)\n",
    "\n",
    "    return mutated\n",
    "\n",
    "\n",
    "def calc_cost(m, X, y):\n",
    "    '''\n",
    "    Calculates the cost for given X, y and vector total (m)\n",
    "    \n",
    "    m = total vector of thetas\n",
    "    X = Row of X's np.zeros((2,j))\n",
    "    y = Actual y's np.zeros((2,1))\n",
    "    '''\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        squared_error = (y[i] - X[i]) ** 2\n",
    "        total += squared_error\n",
    "    \n",
    "    return (1/(2*m)) * total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for hypotesis  1.0  is  0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "# calculate cost for hypothesis\n",
    "hyp_values = multiply_matrix(X, hyps)\n",
    "\n",
    "print(\"Cost for hypotesis \", hyps, \" is \", calc_cost(len(X), y, hyp_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "_Gradient Descent is the method by which we make our model learn._\n",
    "\n",
    "![png](reports/gradient_decent.gif)\n",
    "\n",
    "gratient_cost.png\n",
    "\n",
    "- É algoritmo de otimização para encontrar _local/global minima_\n",
    "- É recomendado para casos de com > 100 features\n",
    "\n",
    "![png](reports/less_100.png)\n",
    "\n",
    "Menos que isso, a sugestãõ é usar algo analítico.\n",
    "\n",
    "- Easy to implement\n",
    "- Requires less memory and processing\n",
    "\n",
    "Exemplo da montanha\n",
    "- imagine que vc esta no alto de uma montanha e deseja descer de olhos fechados.\n",
    "- descer é equivalente a aproximar a reta dos pontos.\n",
    "![png](reports/gradient_descent.png)\n",
    "\n",
    "![png](reports/gradient_descent_2.png)\n",
    "\n",
    "![png](reports/gradient_descent_3.png)\n",
    "\n",
    "#### Graphic\n",
    "- Para minimizar os erros basta calcular a derivada da função de erro.\n",
    "- Use a regra da cadeia\n",
    "- Quanto mais baixo estiver o valor de _error_, menor é a taxa de aprendizagem α.\n",
    "\n",
    "\n",
    "![png](reports/gardient_descent_mat.png)\n",
    "\n",
    "![png](reports/cost_fucntion.png)\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "![png](reports/random.gif)\n",
    "\n",
    "1. Insira parâmetros aleatórios\n",
    "2. Calcule a _cost fucntion_\n",
    "3. Aplique a regra da cadeia na _cost function_ até encontrar o menor valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 3\n",
    "learning_rate = 0.01 # Learning rate\n",
    "precision = 0.000001 # Stop loop\n",
    "previous_step_size = 1 \n",
    "max_iters = 10000 # Maximum number of iterations\n",
    "count = 0 # Iteration counter\n",
    "\n",
    "# derivate cost function\n",
    "derivative_function = lambda x: 2*(x+5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local minimum at -4.9999518490318176\n"
     ]
    }
   ],
   "source": [
    "while previous_step_size > precision and count < max_iters:\n",
    "    current_x = X\n",
    "    X = X - learning_rate * derivative_function(current_x) # Grad descent\n",
    "    \n",
    "    previous_step_size = abs(X - current_x)\n",
    "    count = count + 1\n",
    "    # print(\"Iteration\",count,\"\\nX value is\",X) \n",
    "    \n",
    "print(\"The local minimum at\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regressions Types\n",
    "Até 100k linhas pode ser usado os modelos abaixo, acima disso é recomendado reanalisar o modelo linear.\n",
    "\n",
    "\n",
    "## Polinomial Regression\n",
    "\n",
    "![gif](reports/polinomial_regression.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "_Regularization basically adds the penalty as model complexity increases._\n",
    "\n",
    "- É uma técnica para evitar overfit\n",
    "- Tranforma modelos complexos em modelos mais simples\n",
    "- É usado somente para regressão polinomial\n",
    "- Ex\n",
    "\n",
    "![png](reports/regularizaion.png)\n",
    "\n",
    "#### Complexity Calculate\n",
    "Para calcular qual modelo generaliza melhor é preciso calcular a complexidade:\n",
    "\n",
    "```\n",
    "complexity = error + coefficients\n",
    "```\n",
    "\n",
    "![png](reports/regularization_2.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Rugularization Types\n",
    "\n",
    "Há 2 tipos de regularização\n",
    "- Lasso - L1\n",
    "\n",
    "![png](reports/regularization_ex_1.png)\n",
    "\n",
    "![png](reports/regularization_ex_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rigde - L2\n",
    "\n",
    "\n",
    "![png](reports/l2_reg.png)\n",
    "\n",
    "\n",
    "\n",
    "![png](reports/l2_reg_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![png](reports/sheet_cheat_regu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparation Linear, Lasso and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "linear_regression_regularization_l1 = Lasso(alpha=0.1)\n",
    "linear_regression_regularization_l1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "linear_regression_regularization_l2 = Ridge(alpha=0.1)\n",
    "linear_regression_regularization_l2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_regularization_l1.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95454545])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_regularization_l2.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### References\n",
    "- [1] Material da disciplina de Técnicas Estatísticas de Prediçao:https://moodle.ufsc.br/pluginfile.php/1592338/mod_resource/content/1/An%C3%A1lise%20da%20regres%C3%A3o.pdf\n",
    "- [2] https://www.coursera.org/learn/machine-learning/\n",
    "- [3] https://towardsdatascience.com/implement-gradient-descent-in-python-9b93ed7108d1\n",
    "- [4] https://en.wikipedia.org/wiki/Gradient_descent\n",
    "- [5] https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "- [6] https://matheusfacure.github.io/2017/02/16/ols-mechanics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
